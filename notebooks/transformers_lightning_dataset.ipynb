{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cmBBaVD0LQxr"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet numpy scipy scikit-learn lightning transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hqdc0fpkQAOi"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lQtnpJX449V",
    "outputId": "72069f6d-6770-4271-8d72-b8cd76aea01c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Some fo the training inputs are too large, this is a hackish solution, should resort to limiting the file column size\n",
    "# More info here: https://stackoverflow.com/questions/54042406/error-field-larger-than-field-limit-131072\n",
    "csv.field_size_limit(256<<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "voK5q8t052Ua"
   },
   "outputs": [],
   "source": [
    "DATASET = '../data/casimedicos/dev_relations.tsv'\n",
    "MODEL = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtNtqi576oE1"
   },
   "source": [
    "# PyTorch Dataset for Sequence Classification\n",
    "\n",
    "The following is the prototype for a dataset class for Sequence Classification. I am not quite convinced that the tokenizer should be part of this, but in this way we do not have to store the whole padded data at once (unlike the version showed in [Lightining's documentation](https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/text-transformers.html)).\n",
    "\n",
    "As a drawback, in this version we cannot really benefit from the **Fast Tokenizers**, since there's a warning that using a Fast Tokenizer with a collator function for padding after is slower than processing the whole batch using the tokenizer call. However, if we have that version, we will have to pad/tokenize everything in advance which can be memory consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bx0Sakcb47rp"
   },
   "outputs": [],
   "source": [
    "class SentenceClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TODO: Docstring. Explain the way the dataset is expected (label first with the __label__ at the beginning)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer_model_or_path: str,\n",
    "                 path_to_dataset: Optional[str] = None,\n",
    "                 dataset: Optional[List[Tuple[str, str, str]]] = None,\n",
    "                 labels: Optional[Dict[str, int]] = None,\n",
    "                 delimiter: str = '\\t',\n",
    "                 quotechar: str = '\"'):\n",
    "        if path_to_dataset is not None and dataset is not None:\n",
    "            logger.warn(\"Both path and dataset were provided. Ignoring the path, using the parsed dataset.\")\n",
    "            path_to_dataset = None\n",
    "        elif path_to_dataset is None and dataset is None:\n",
    "            raise ValueError(\"Provide either path to a file or a dataset as a list of tuples\")\n",
    "\n",
    "        if path_to_dataset is not None:\n",
    "            with open(path_to_dataset, \"rt\") as fh:\n",
    "                csv_reader = csv.reader(fh, delimiter=delimiter, quotechar=quotechar)\n",
    "                dataset = list(csv_reader)\n",
    "\n",
    "        target = [d[0].lstrip('__label__') for d in dataset]  # TODO: this assumes the dataset was not parsed with this same function\n",
    "\n",
    "        self.labels = labels if labels is not None else {lbl: idx for idx, lbl in enumerate(sorted(set(target)))}  # TODO: This is a problem if we have 3 different splits and not all labels appear in all splits\n",
    "        self.dataset = [\n",
    "            {\n",
    "                \"text\": d[1],\n",
    "                \"text_pair\": d[2]\n",
    "            }\n",
    "            for d, t in zip(dataset, target)\n",
    "        ]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_or_path, use_fast=False)\n",
    "        self.target = [self.labels[t] for t in target]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "\n",
    "        # WARNING: This does not work with a range of values (if idx is a range instead of a single value)\n",
    "        tokenized_data = self.tokenizer(**data, truncation=True)\n",
    "        tokenized_data['label'] = self.target[idx]\n",
    "\n",
    "        return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KFuMlwT70G-"
   },
   "source": [
    "As you can see, now we can just use the SentenceClassificationDataset class as is in the DataLoader along a DataCollatorWithPadding and the DataLoader will pad the batch and in general will give us the batch that can be directly used over the Transformer Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3BpHsXyQCCV",
    "outputId": "8d7bf940-0f79-4d67-9c75-3088edfe0cca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,    62, 46667,  ...,     1,     1,     1],\n",
       "        [    0,    62, 46667,  ...,     1,     1,     1],\n",
       "        [    0,    62, 46667,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  4687,  1556,  ...,     1,     1,     1],\n",
       "        [    0,  4687,  1556,  ...,     1,     1,     1],\n",
       "        [    0,  4687,  1556,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 1, 2, 2, 2])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SentenceClassificationDataset(MODEL, DATASET)\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(dataset.tokenizer)\n",
    ")\n",
    "batch_input = next(iter(loader))\n",
    "batch_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LestAfGD8En-"
   },
   "source": [
    "There's no need for extra steps and we have the loss for this specific batch of data already implemented within the model, we can directly use that as the loss function for the `LightningModule`'s `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIHKKSIR5GWO",
    "outputId": "48c9b991-d635-4f03-e531-1aa1ee85ffcb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.1785, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0862,  0.0999, -0.0301],\n",
       "        [ 0.0893,  0.1055, -0.0283],\n",
       "        [ 0.0911,  0.1032, -0.0371],\n",
       "        [ 0.0872,  0.1015, -0.0356],\n",
       "        [ 0.0882,  0.1011, -0.0362],\n",
       "        [ 0.0905,  0.1023, -0.0377],\n",
       "        [ 0.0887,  0.1025, -0.0408],\n",
       "        [ 0.0889,  0.1024, -0.0390],\n",
       "        [ 0.0859,  0.1052, -0.0344],\n",
       "        [ 0.0893,  0.1045, -0.0368],\n",
       "        [ 0.0848,  0.1048, -0.0362],\n",
       "        [ 0.0841,  0.1062, -0.0274],\n",
       "        [ 0.0832,  0.1102, -0.0211],\n",
       "        [ 0.0919,  0.1011, -0.0368],\n",
       "        [ 0.0818,  0.1074, -0.0355],\n",
       "        [ 0.0850,  0.1069, -0.0297],\n",
       "        [ 0.0847,  0.1063, -0.0300],\n",
       "        [ 0.0904,  0.1054, -0.0337],\n",
       "        [ 0.0900,  0.0977, -0.0301],\n",
       "        [ 0.0921,  0.0985, -0.0345],\n",
       "        [ 0.0914,  0.1070, -0.0391],\n",
       "        [ 0.0864,  0.1073, -0.0318],\n",
       "        [ 0.0832,  0.1102, -0.0211],\n",
       "        [ 0.0883,  0.1073, -0.0415],\n",
       "        [ 0.0872,  0.1015, -0.0356],\n",
       "        [ 0.0882,  0.1011, -0.0362],\n",
       "        [ 0.0905,  0.1023, -0.0377],\n",
       "        [ 0.0887,  0.1025, -0.0408],\n",
       "        [ 0.0833,  0.1056, -0.0346],\n",
       "        [ 0.0892,  0.1052, -0.0355],\n",
       "        [ 0.0928,  0.1034, -0.0311],\n",
       "        [ 0.0865,  0.1046, -0.0254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL, num_labels=len(dataset.labels), label2id=dataset.labels, id2label={idx: lbl for lbl, idx in dataset.labels.items()})\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, config=config)\n",
    "model(**batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrJ2x1yMQqwx"
   },
   "source": [
    "# Pytorch Dataset for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FVFdJpdAQqwy"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoConfig, AutoModelForTokenClassification\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-XWvjGX7Qqwy"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yi2XaEt4Qqwy"
   },
   "outputs": [],
   "source": [
    "DATASET = '../data/casimedicos/dev_revisited.conll'\n",
    "MODEL = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PISL1oMbQqwy"
   },
   "outputs": [],
   "source": [
    "class SentenceClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TODO: Docstring. Explain the way the dataset is expected (label first with the __label__ at the beginning)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer_model_or_path: str,\n",
    "                 path_to_dataset: Optional[str] = None,\n",
    "                 dataset: Optional[List[Tuple[str, str, str]]] = None,\n",
    "                 labels: Optional[Dict[str, int]] = None,\n",
    "                 delimiter: str = '\\t',\n",
    "                 token_position: int = 1,\n",
    "                 label_position: int = 4):\n",
    "        if path_to_dataset is not None and dataset is not None:\n",
    "            logger.warn(\"Both path and dataset were provided. Ignoring the path, using the parsed dataset.\")\n",
    "            path_to_dataset = None\n",
    "        elif path_to_dataset is None and dataset is None:\n",
    "            raise ValueError(\"Provide either path to a file or a dataset as a list of tuples\")\n",
    "\n",
    "        if path_to_dataset is not None:\n",
    "            sentences = self._load_conll_sentences(path_to_dataset, delimiter, token_position, label_position)\n",
    "\n",
    "        # TODO: What happens if not all the labels are present? This might be better to do outside here\n",
    "        if labels is None:\n",
    "            self.labels = {lbl: idx for idx, lbl in enumerate(sorted(set(chain(*map(itemgetter('labels'), sentences)))))}\n",
    "        else:\n",
    "            self.labels = labels\n",
    "\n",
    "        self.dataset = [\n",
    "            {\n",
    "                \"tokens\": sentence['tokens'],\n",
    "                \"labels\": [self.labels[l] for l in sentence['labels']]\n",
    "            } for sentence in sentences\n",
    "        ]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_or_path, use_fast=False)\n",
    "\n",
    "    def _load_conll_sentences(self, path_to_dataset, delimiter='\\t', token_position=1, label_position=4):\n",
    "        with open(path_to_dataset, 'rt') as fh:\n",
    "            sentences = []\n",
    "            sentence_tokens = []\n",
    "            sentence_labels = []\n",
    "            for line in fh:\n",
    "                line = line.strip().split(delimiter)\n",
    "\n",
    "                if len(line) < 2:\n",
    "                    # We have the end of a sentence\n",
    "                    assert len(sentence_tokens) == len(sentence_labels)\n",
    "                    if len(sentence_tokens) == 0:\n",
    "                        # Happens after a paragraph change (there are 2 blank lines)\n",
    "                        continue\n",
    "\n",
    "                    sentences.append({\n",
    "                        \"tokens\": sentence_tokens,\n",
    "                        \"labels\": sentence_labels\n",
    "                    })\n",
    "                    sentence_tokens = []\n",
    "                    sentence_labels = []\n",
    "                else:\n",
    "                    sentence_tokens.append(line[token_position])\n",
    "                    sentence_labels.append(line[label_position])\n",
    "        return sentences\n",
    "\n",
    "    def _tokenize_and_align_labels(self, sentence):\n",
    "        tokenized_sentence = self.tokenizer(sentence[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        sentence_labels = []\n",
    "        word_ids = tokenized_sentence.word_ids()\n",
    "        previous_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                # Assing the -100 label to the special characters (e.g. [CLS], [SEP], etc.)\n",
    "                # Why the -100? Because Pytorch CrossEntropy loss ignore this.\n",
    "                sentence_labels.append(-100)\n",
    "            elif wid != previous_wid:\n",
    "                # Only label the first token of a given word\n",
    "                # This is according to https://huggingface.co/docs/transformers/tasks/token_classification#train\n",
    "                # Maybe a better way would be to replicate label? It might be good that this is configurable\n",
    "                sentence_labels.append(sentence[\"labels\"][wid])\n",
    "            else:\n",
    "                # The subtokens don't have a valid label.\n",
    "                # Again, this perhaps should be confiurable.\n",
    "                sentence_labels.append(-100)\n",
    "            previous_wid = wid\n",
    "\n",
    "        tokenized_sentence['labels'] = sentence_labels\n",
    "\n",
    "        assert len(tokenized_sentence['input_ids']) == len(tokenized_sentence['labels'])\n",
    "\n",
    "        return tokenized_sentence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._tokenize_and_align_labels(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3nhFS7n5Qqwz"
   },
   "outputs": [],
   "source": [
    "dataset = SentenceClassificationDataset(MODEL, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTs6xOgqQqwz",
    "outputId": "7fd78570-1b50-4a9d-bcc6-f9cedee64328"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 21389,   294,  ...,     1,     1,     1],\n",
       "        [    0,  4865,    83,  ...,     1,     1,     1],\n",
       "        [    0,   116,    20,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,    20,   984,  ...,     1,     1,     1],\n",
       "        [    0, 21389,   294,  ...,     1,     1,     1],\n",
       "        [    0,  4865,  9060,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,   10, -100,  ..., -100, -100, -100],\n",
       "        [-100,   10,   10,  ..., -100, -100, -100],\n",
       "        [-100,   10,   10,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,   10,    0,  ..., -100, -100, -100],\n",
       "        [-100,   10, -100,  ..., -100, -100, -100],\n",
       "        [-100,   10,   10,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorForTokenClassification(dataset.tokenizer)\n",
    ")\n",
    "batch_input = next(iter(loader))\n",
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7_LEMUGQqw0",
    "outputId": "0f3503a3-bea0-4c9f-b416-5e7b3693db27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(2.0128, grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.3063, -0.2447, -0.0432,  ..., -0.2811,  0.1820, -0.1890],\n",
       "         [ 0.2146, -0.5982,  0.0442,  ..., -0.3867,  0.2949, -0.2873],\n",
       "         [ 0.2043, -0.6358,  0.0644,  ..., -0.3147,  0.2378, -0.4256],\n",
       "         ...,\n",
       "         [ 0.3311, -0.2030, -0.0354,  ..., -0.3211,  0.1602, -0.1613],\n",
       "         [ 0.3311, -0.2030, -0.0354,  ..., -0.3211,  0.1602, -0.1613],\n",
       "         [ 0.3311, -0.2030, -0.0354,  ..., -0.3211,  0.1602, -0.1613]],\n",
       "\n",
       "        [[ 0.3273, -0.0974,  0.0015,  ..., -0.2431,  0.1527, -0.1260],\n",
       "         [ 0.2783, -0.5119,  0.0414,  ..., -0.4060,  0.2896, -0.2661],\n",
       "         [ 0.2476, -0.5478, -0.0046,  ..., -0.4702,  0.2320, -0.3108],\n",
       "         ...,\n",
       "         [ 0.2645, -0.5441,  0.0147,  ..., -0.4056,  0.2817, -0.4508],\n",
       "         [ 0.2645, -0.5441,  0.0147,  ..., -0.4056,  0.2817, -0.4508],\n",
       "         [ 0.2645, -0.5441,  0.0147,  ..., -0.4056,  0.2817, -0.4508]],\n",
       "\n",
       "        [[ 0.2776, -0.0603,  0.0315,  ..., -0.1715,  0.1471, -0.1995],\n",
       "         [ 0.0975, -0.6014,  0.0754,  ..., -0.4418,  0.3232, -0.3237],\n",
       "         [ 0.1406, -0.6131,  0.1066,  ..., -0.3859,  0.2993, -0.2942],\n",
       "         ...,\n",
       "         [ 0.2633, -0.5194,  0.0558,  ..., -0.3672,  0.3037, -0.4437],\n",
       "         [ 0.2633, -0.5194,  0.0558,  ..., -0.3672,  0.3037, -0.4437],\n",
       "         [ 0.2633, -0.5194,  0.0558,  ..., -0.3672,  0.3037, -0.4437]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2847, -0.2257, -0.0167,  ..., -0.2754,  0.1958, -0.1868],\n",
       "         [ 0.0961, -0.5931,  0.0737,  ..., -0.3758,  0.2512, -0.3491],\n",
       "         [ 0.2452, -0.5794,  0.0723,  ..., -0.3711,  0.2422, -0.3255],\n",
       "         ...,\n",
       "         [ 0.3087, -0.1797, -0.0008,  ..., -0.3224,  0.1735, -0.1520],\n",
       "         [ 0.3087, -0.1797, -0.0008,  ..., -0.3224,  0.1735, -0.1520],\n",
       "         [ 0.3087, -0.1797, -0.0008,  ..., -0.3224,  0.1735, -0.1520]],\n",
       "\n",
       "        [[ 0.3190, -0.2503, -0.0304,  ..., -0.2949,  0.1879, -0.2016],\n",
       "         [ 0.2184, -0.6073,  0.0533,  ..., -0.3762,  0.2948, -0.3106],\n",
       "         [ 0.1937, -0.6387,  0.0602,  ..., -0.3192,  0.2202, -0.4650],\n",
       "         ...,\n",
       "         [ 0.3454, -0.2181, -0.0193,  ..., -0.3457,  0.1666, -0.1740],\n",
       "         [ 0.3454, -0.2181, -0.0193,  ..., -0.3457,  0.1666, -0.1740],\n",
       "         [ 0.3454, -0.2181, -0.0193,  ..., -0.3457,  0.1666, -0.1740]],\n",
       "\n",
       "        [[ 0.3250, -0.1123, -0.0598,  ..., -0.2074,  0.1725, -0.1378],\n",
       "         [ 0.3029, -0.5576,  0.0164,  ..., -0.4077,  0.2657, -0.3219],\n",
       "         [ 0.2726, -0.5682,  0.0586,  ..., -0.4049,  0.1878, -0.3813],\n",
       "         ...,\n",
       "         [ 0.2573, -0.5484, -0.0023,  ..., -0.4003,  0.2863, -0.4582],\n",
       "         [ 0.2573, -0.5484, -0.0023,  ..., -0.4003,  0.2863, -0.4582],\n",
       "         [ 0.2573, -0.5484, -0.0023,  ..., -0.4003,  0.2863, -0.4582]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL, num_labels=len(dataset.labels), label2id=dataset.labels, id2label={idx: lbl for lbl, idx in dataset.labels.items()})\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL, config=config)\n",
    "model(**batch_input)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
