{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cmBBaVD0LQxr"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet numpy scipy scikit-learn lightning transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hqdc0fpkQAOi"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lQtnpJX449V",
    "outputId": "0ee1d19e-21f7-4f7d-fc99-67840133158c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Some fo the training inputs are too large, this is a hackish solution, should resort to limiting the file column size\n",
    "# More info here: https://stackoverflow.com/questions/54042406/error-field-larger-than-field-limit-131072\n",
    "csv.field_size_limit(256<<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "voK5q8t052Ua"
   },
   "outputs": [],
   "source": [
    "DATASET = '../data/casimedicos/dev_relations.tsv'\n",
    "MODEL = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtNtqi576oE1"
   },
   "source": [
    "# PyTorch Dataset for Sequence Classification\n",
    "\n",
    "The following is the prototype for a dataset class for Sequence Classification. I am not quite convinced that the tokenizer should be part of this, but in this way we do not have to store the whole padded data at once (unlike the version showed in [Lightining's documentation](https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/text-transformers.html)).\n",
    "\n",
    "As a drawback, in this version we cannot really benefit from the **Fast Tokenizers**, since there's a warning that using a Fast Tokenizer with a collator function for padding after is slower than processing the whole batch using the tokenizer call. However, if we have that version, we will have to pad/tokenize everything in advance which can be memory consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bx0Sakcb47rp"
   },
   "outputs": [],
   "source": [
    "class SentenceClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TODO: Docstring. Explain the way the dataset is expected (label first with the __label__ at the beginning)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer_model_or_path: str,\n",
    "                 path_to_dataset: Optional[str] = None,\n",
    "                 dataset: Optional[List[Tuple[str, str, str]]] = None,\n",
    "                 labels: Optional[Dict[str, int]] = None,\n",
    "                 delimiter: str = '\\t',\n",
    "                 quotechar: str = '\"'):\n",
    "        if path_to_dataset is not None and dataset is not None:\n",
    "            logger.warn(\"Both path and dataset were provided. Ignoring the path, using the parsed dataset.\")\n",
    "            path_to_dataset = None\n",
    "        elif path_to_dataset is None and dataset is None:\n",
    "            raise ValueError(\"Provide either path to a file or a dataset as a list of tuples\")\n",
    "\n",
    "        if path_to_dataset is not None:\n",
    "            with open(path_to_dataset, \"rt\") as fh:\n",
    "                csv_reader = csv.reader(fh, delimiter=delimiter, quotechar=quotechar)\n",
    "                dataset = list(csv_reader)\n",
    "\n",
    "        target = [d[0].lstrip('__label__') for d in dataset]\n",
    "\n",
    "        self.labels = labels if labels is not None else {lbl: idx for idx, lbl in enumerate(sorted(set(target)))}\n",
    "        self.dataset = [\n",
    "            {\n",
    "                \"text\": d[1],\n",
    "                \"text_pair\": d[2]\n",
    "            }\n",
    "            for d, t in zip(dataset, target)\n",
    "        ]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_or_path, use_fast=False)\n",
    "        self.target = [self.labels[t] for t in target]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "\n",
    "        # WARNING: This does not work with a range of values (if idx is a range instead of a single value)\n",
    "        tokenized_data = self.tokenizer(**data, truncation=True)\n",
    "        tokenized_data['label'] = self.target[idx]\n",
    "\n",
    "        return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KFuMlwT70G-"
   },
   "source": [
    "As you can see, now we can just use the SentenceClassificationDataset class as is in the DataLoader along a DataCollatorWithPadding and the DataLoader will pad the batch and in general will give us the batch that can be directly used over the Transformer Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3BpHsXyQCCV",
    "outputId": "77f67acb-bb69-4b59-b868-b4a3d79348d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,    62, 46667,  ...,     1,     1,     1],\n",
       "        [    0,    62, 46667,  ...,     1,     1,     1],\n",
       "        [    0,    62, 46667,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  4687,  1556,  ...,     1,     1,     1],\n",
       "        [    0,  4687,  1556,  ...,     1,     1,     1],\n",
       "        [    0,  4687,  1556,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 1, 2, 2, 2])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SentenceClassificationDataset(MODEL, DATASET)\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(dataset.tokenizer)\n",
    ")\n",
    "batch_input = next(iter(loader))\n",
    "batch_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LestAfGD8En-"
   },
   "source": [
    "There's no need for extra steps and we have the loss for this specific batch of data already implemented within the model, we can directly use that as the loss function for the `LightningModule`'s `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIHKKSIR5GWO",
    "outputId": "387ca5f2-cd4b-4dc3-f589-7d4a7db29f2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.0986, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1783,  0.1205, -0.0246],\n",
       "        [-0.1729,  0.1264, -0.0276],\n",
       "        [-0.1686,  0.1263, -0.0224],\n",
       "        [-0.1696,  0.1244, -0.0245],\n",
       "        [-0.1716,  0.1232, -0.0256],\n",
       "        [-0.1682,  0.1239, -0.0219],\n",
       "        [-0.1661,  0.1245, -0.0228],\n",
       "        [-0.1679,  0.1249, -0.0238],\n",
       "        [-0.1686,  0.1274, -0.0237],\n",
       "        [-0.1711,  0.1278, -0.0234],\n",
       "        [-0.1697,  0.1281, -0.0227],\n",
       "        [-0.1713,  0.1248, -0.0297],\n",
       "        [-0.1780,  0.1245, -0.0307],\n",
       "        [-0.1743,  0.1194, -0.0293],\n",
       "        [-0.1677,  0.1249, -0.0304],\n",
       "        [-0.1732,  0.1279, -0.0274],\n",
       "        [-0.1726,  0.1263, -0.0312],\n",
       "        [-0.1787,  0.1183, -0.0262],\n",
       "        [-0.1804,  0.1296, -0.0282],\n",
       "        [-0.1767,  0.1230, -0.0294],\n",
       "        [-0.1701,  0.1282, -0.0272],\n",
       "        [-0.1727,  0.1273, -0.0281],\n",
       "        [-0.1780,  0.1245, -0.0307],\n",
       "        [-0.1645,  0.1210, -0.0294],\n",
       "        [-0.1696,  0.1244, -0.0245],\n",
       "        [-0.1716,  0.1232, -0.0256],\n",
       "        [-0.1682,  0.1239, -0.0219],\n",
       "        [-0.1661,  0.1245, -0.0228],\n",
       "        [-0.1670,  0.1243, -0.0362],\n",
       "        [-0.1664,  0.1271, -0.0249],\n",
       "        [-0.1727,  0.1233, -0.0282],\n",
       "        [-0.1776,  0.1167, -0.0368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL, num_labels=len(dataset.labels), label2id=dataset.labels, id2label={idx: lbl for lbl, idx in dataset.labels.items()})\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, config=config)\n",
    "model(**batch_input)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
